{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: [DistilledData](https://github.com/DistilledData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\tableofcontents\n",
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T18:43:03.335303Z",
     "start_time": "2020-10-29T18:43:03.333503Z"
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook continues the discussion of probability from \"Advanced_Probabiliity_Basics.\" In particular, this notebook will focus continue our discussion of expectation with mean, variance and covariance. This notebook will end with a brief discussion of the indicator function. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T03:31:17.128828Z",
     "start_time": "2020-11-01T03:31:17.125135Z"
    }
   },
   "source": [
    "Given a set of data $x_1,\\cdots,x_n$, the sample mean $\\bar{x}$ is given by\n",
    "$$\\bar{x}=\\frac{1}{n}\\sum\\limits_{i=1}^nx_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the mean for a random variable $X$ is given by $\\mathbb{E}[X]$, I will refer the reader to the discussion on expected value and its properties in the \"Advanced_Probability_Basics\" notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between Sample and Population Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If each $x_i$ in a data set corresponds to the outcome of a trial for the random variable X, and if each $x_i$ is independently drawn, then we expect sample mean $\\bar{x}$ to approach the theoretical mean $\\mathbb{E}[X]$ as we continue to draw more and more samples. In other words,\n",
    "$$\\lim\\limits_{n\\to\\infty}\\bar{x}=\\mathbb{E}[X]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sample Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informally, given a particular set of data, variance measures the spread of the data. More formally, given a set of data $x_1,\\cdots,x_n$ with mean $\\bar{x}$, the sample variance is\n",
    "$$S^2=\\frac{\\sum\\limits_{i=1}^n(x_i-\\bar{x})^2}{n-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also measure variance for random variables! Given random variable $X$ with mean $\\mathbb{E}[X]$, the variance is defined below\n",
    "$$\\text{Var}(X)=\\mathbb{E}[(X-\\mathbb{E}[X])^2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between Sample and Population Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T18:54:55.438293Z",
     "start_time": "2020-10-29T18:54:55.434353Z"
    }
   },
   "source": [
    "If the $x_i$ in the data above are an outcome in the outcome space for $X$ such that each $x_i$ are drawn independently, then we find that\n",
    "$$\\lim\\limits_{n\\to\\infty}S^2=\\text{Var}(X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Var}(X)=\\mathbb{E}[X^2]-\\mathbb{E}[X]^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{aligned} \n",
    "\\text{Var}(X)&=\\mathbb{E}[(X-\\mathbb{E}[X])^2=\\mathbb{E}\\big[X^2-2X\\mathbb{E}[X]+\\mathbb{E}[X]^2\\big] \\\\\n",
    "&=\\mathbb{E}[X^2]-2\\mathbb{E}[X]^2+\\mathbb{E}[X]^2=\\mathbb{E}[X^2]-\\mathbb{E}[X]^2\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Mathematical Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume real-valued random variables $X$ and $Y$ with means $\\mu_X$ and $\\mu_Y$. We will also assume some $a,b\\in\\mathbb{R}$. Properties 4 and 5 use the definition of covariance that we define further below in this notebook.\n",
    "\n",
    "1. $\\text{Var}(a)=0$\n",
    "2. $\\text{Var}(X+a)=\\text{Var}(X)$\n",
    "3. $\\text{Var}(aX)=a^2\\text{Var}(X)$\n",
    "4. $\\text{Var}(aX+bY)=a^2\\text{Var}(X)+b^2\\text{Var}(Y)+2ab\\text{Cov}(X,Y)$\n",
    "5. $\\text{Var}(aX-bY)=a^2\\text{Var}(X)+b^2\\text{Var}(Y)-2ab\\text{Cov}(X,Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T02:37:43.893979Z",
     "start_time": "2020-11-02T02:37:43.891992Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Property 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\text{Var}(a)=\\mathbb{E}[a^2]-\\mathbb{E}[a]^2=a^2-a^2=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Property 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{aligned}\n",
    "\\text{Var}(aX)&=\\mathbb{E}[(aX)^2]-\\mathbb{E}[aX]^2=\\mathbb{E}[(aX)^2]-(a\\mathbb{E}[X])^2 \\\\\n",
    "&=\\mathbb{E}[a^2X^2]-a^2\\mathbb{E}[X]^2=a^2\\mathbb{E}[X^2]-a^2\\mathbb{E}[X]^2 \\\\\n",
    "&=a^2(\\mathbb{E}[X^2]-\\mathbb{E}[X]^2)=a^2\\text{Var}(X)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Property 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{aligned}\n",
    "\\text{Var}(X+a)&=\\mathbb{E}[(X+a)^2]-\\mathbb{E}[X+a]^2=\\mathbb{E}[(X+a)^2]-(\\mathbb{E}[X]+\\mathbb{E}[a])^2 \\\\\n",
    "&=\\mathbb{E}[(X+a)^2]-(\\mathbb{E}[X]+a)^2 \\\\\n",
    "&=\\mathbb{E}[X^2+2aX+a^2]-(\\mathbb{E}[X]^2+2a\\mathbb{E}[X]+a^2) \\\\\n",
    "&=\\mathbb{E}[X^2]+2a\\mathbb{E}[X]+a^2-(\\mathbb{E}[X]^2+2a\\mathbb{E}[X]+a^2) \\\\\n",
    "&=\\mathbb{E}[X^2]-\\mathbb{E}[X]^2=\\text{Var}(X)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Property 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{aligned}\n",
    "\\text{Var}(aX+bY)&=\\mathbb{E}[(aX+bY)^2]-\\mathbb{E}[aX+bY]^2 \\\\\n",
    "&=\\mathbb{E}[(a^2X^2+2abXY+b^2Y^2]-(a\\mathbb{E}[X]+b\\mathbb{E}[Y])^2 \\\\\n",
    "&=a^2\\mathbb{E}[X^2]+2ab\\mathbb{E}[XY]+b^2\\mathbb{E}[Y^2]-(a^2\\mathbb{E}[X]^2+2ab\\mathbb{E}[X]\\mathbb{E}[Y]+b^2\\mathbb{E}[Y]^2) \\\\\n",
    "&=a^2(\\mathbb{E}[X^2]-\\mathbb{E}[X]^2)+b^2(\\mathbb{E}[Y^2]-\\mathbb{E}[Y]^2)+2ab(\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]) \\\\\n",
    "&=a^2\\text{Var}(X)+b^2\\text{Var}(Y)+2ab\\text{Cov}(X,Y) \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Property 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{aligned}\n",
    "\\text{Var}(aX-bY)&=\\mathbb{E}[(aX-bY)^2]-\\mathbb{E}[aX-bY]^2 \\\\\n",
    "&=\\mathbb{E}[(a^2X^2-2abXY+b^2Y^2]-(a\\mathbb{E}[X]+b\\mathbb{E}[Y])^2 \\\\\n",
    "&=a^2\\mathbb{E}[X^2]-2ab\\mathbb{E}[XY]+b^2\\mathbb{E}[Y^2]-(a^2\\mathbb{E}[X]^2-2ab\\mathbb{E}[X]\\mathbb{E}[Y]+b^2\\mathbb{E}[Y]^2) \\\\\n",
    "&=a^2(\\mathbb{E}[X^2]-\\mathbb{E}[X]^2)+b^2(\\mathbb{E}[Y^2]-\\mathbb{E}[Y]^2)-2ab(\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]) \\\\\n",
    "&=a^2\\text{Var}(X)+b^2\\text{Var}(Y)-2ab\\text{Cov}(X,Y) \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negativity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a real-valued random variable $X$, $\\text{Var}(X)\\ge 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\text{Var}(X)=\\mathbb{E}[(X-\\mathbb{E}[X])^2]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose $X$ is a real-valued continuous random variable with density function $f_X$ and mean $\\mu_X$.\n",
    "\n",
    "$\\text{Var}(X)=\\int_\\mathbb{R}(x-\\mu_X)^2f_X(x)dx$\n",
    "\n",
    "$\\forall x\\in\\mathbb{R}$ $(x-\\mu_X)^2\\ge 0$ and $f_X(x)\\ge0\\implies \\int_\\mathbb{R}(X-\\mu_X)^2f_X(x)dx\\ge\\int_\\mathbb{R}0dx=0$ \n",
    "\n",
    "Therefore, the integral and hence $\\text{Var}(X)$ will also be non-negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose $X$ is a real-valued discrete random variable with PMF $p_X$ and mean $\\mu_X$ <br>\n",
    "$\\text{Var}(X)=\\sum\\limits_x(X-\\mu_X)^2P(X=x)$\n",
    "\n",
    "For all $x$ in the sum above, $(X-\\mu_X)^2\\ge0$ and $P(X=x)\\ge0$, and hence $(X-\\mu_X)^2P(X=0)\\ge0$. Since all terms in the above sum is greater or equal to 0, $\\sum\\limits_x(X-\\mu_X)^2P(X=x)=\\text{Var}(X)\\ge0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance of 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Var}(X)=0\\iff\\exists a$ such that $P(X=a)=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\Longrightarrow$:\n",
    "\n",
    "$\\text{Var}(X)=\\mathbb{E}[(X-\\mathbb{E}[X])^2]=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose X is a continuous random variable with density function $f_X$ and mean $\\mu_X$.\n",
    "\n",
    "$\\text{Var}(X)=\\mathbb{E}[(X-\\mathbb{E}[X])^2]=\\int_\\mathbb{R}(x-\\mu_X)^2f_X(x)dx=0$\n",
    "\n",
    "The integral is 0 $\\iff$ the integrand $(x-\\mu_X)^2f_X(x)$ is uniformly 0. However, there must be some regions where the density $f_X$ is non-zero, otherwise the laws of probability will be violated. $\\forall x\\in\\mathbb{R}$ such that $f_X(x)\\ne 0$, $x=\\mu_X$ in order for the integrand to be zero. However, since $\\mu_X$ is a constant, there must only be one such value $x\\in\\mathbb{R}$ where $f_X$ is non-zero. From our discussion of continuous random variables in the \"Advanced_Probability_Basics\" notebook, we know that one of the properties for a continuous random variable is $\\forall x\\in\\mathbb{R}$ $P(X=x)=0$, which means that we must be working with a discrete random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose X is a discrete random variable with PMF $p_X$ and mean $\\mu_X$.\n",
    "\n",
    "$\\text{Var}(X)=\\mathbb{E}[(X-\\mathbb{E}[X])^2]=\\sum\\limits_x(X-\\mu_X)^2P(X=x)=0$\n",
    "\n",
    "In order for the sum to be 0, each term of the sum must be 0. However, like above, the laws of probability would be violated if $P(X=x)$ were uniformly 0, and hence there is some $x\\in\\mathbb{R}$ where $P(X=x)$. In this instance, in order for the term in the sum to be 0, $X=\\mu_X$. Similar to the argument above, $\\mu_X$ is a constant and hence $\\mu_X=x$ for only one $x\\in\\mathbb{R}$. In order for $p_X$ to be a valid PMF, we require that $p_X(x)=P(X=x)=1$. \n",
    "\n",
    "$\\therefore\\text{Var}(X)=0\\implies\\exists a$ such that $P(X=a)=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\Longleftarrow$:\n",
    "\n",
    "$\\mathbb{E}[X^2]=\\sum\\limits_x P(X=x)x^2=1\\cdot a^2=a^2$\n",
    "\n",
    "$\\mathbb{E}[X]^2=\\bigg(\\sum\\limits_x P(X=x)x\\bigg)^2=a^2$\n",
    "\n",
    "$\\text{Var}(X)=\\mathbb{E}[X^2]-\\mathbb{E}[X]^2=a^2-a^2=0$\n",
    "\n",
    "$\\therefore\\exists a$ such that $P(X=a)=1\\implies\\text{Var}(X)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\therefore\\text{Var}(X)=0\\iff\\exists a$ such that $P(X=a)=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T18:55:27.938674Z",
     "start_time": "2020-10-29T18:55:27.936769Z"
    }
   },
   "source": [
    "# Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given data with two covariates $X$ and $Y$ with values $(x_1,y_1),\\cdots,(x_n,y_n)$, such that the sample mean for $X$ and $Y$ are $\\bar{x}$ and $\\bar{y}$, respectively, the covariances for the distribution is <br>\n",
    "$$\\text{Cov}(x,y)=\\frac{1}{n}\\sum\\limits_i(x_i-\\bar{x})(y_i-\\bar{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T05:05:11.467178Z",
     "start_time": "2020-11-01T05:05:11.463139Z"
    }
   },
   "source": [
    "Intuitively, covariance measures how related one random variable is to another. Given two random variables $X$ and $Y$ that form a [joint distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution) (X,Y), the covariance, often denoted $\\sigma_{XY}$ or $\\sigma(X,Y)$ is given by <br>\n",
    "$$\\text{Cov}(X,Y)=\\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T05:11:08.257247Z",
     "start_time": "2020-11-01T05:11:08.255235Z"
    }
   },
   "source": [
    "## Properties of Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T05:13:42.190949Z",
     "start_time": "2020-11-01T05:13:42.186171Z"
    }
   },
   "source": [
    "$\\text{Cov}(X,Y)=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proof:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{aligned}\n",
    "\\text{Cov}(X,Y)&=\\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])] \\\\\n",
    "&=\\mathbb{E}\\big[XY-Y\\mathbb{E}[X]-X\\mathbb{E}[Y]+\\mathbb{E}[X]\\mathbb{E}[Y]\\big] \\\\\n",
    "&=\\mathbb{E}\\big[XY\\big]-\\mathbb{E}\\big[Y\\mathbb{E}[X]\\big]-\\mathbb{E}\\big[X\\mathbb{E}[Y]\\big]+\\mathbb{E}\\big[\\mathbb{E}[X]\\mathbb{E}[Y]\\big] \\\\\n",
    "&=\\mathbb{E}[XY]-\\mathbb{E}[Y]\\mathbb{E}[X]-\\mathbb{E}[X]\\mathbb{E}[Y]+\\mathbb{E}[X]\\mathbb{E}[Y] \\\\\n",
    "&=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y] \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that $X$, $Y$, $W$, $V$ are real-valued random variables, and $a,b,c,d\\in\\mathbb{R}$\n",
    "\n",
    "1. $\\text{Cov}(X,a)=0$\n",
    "2. $\\text{Cov}(X,Y)=\\text{Cov}(Y,X)$\n",
    "3. $\\text{Cov}(aX,bY)=ab\\text{Cov}(X,Y)$\n",
    "4. $\\text{Cov}(X+a,Y+b)=\\text{Cov}(X,Y)$\n",
    "5. $\\text{Cov}(aX+bY,cW+dV)=ac\\text{Cov}(X,W)+ad\\text{Cov}(X,V)+bc\\text{Cov}(Y,W)+bd\\text{Cov}(Y,V)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Property 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\text{Cov}(X,a)=\\mathbb{E}[X\\cdot a]-\\mathbb{E}[X]\\mathbb{E}[a]=a\\mathbb{E}[X]-a\\mathbb{E}[X]=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Property 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\text{Cov}(X,Y)=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]=\\mathbb{E}[YX]-\\mathbb{E}[Y]\\mathbb{E}[X]=\\text{Cov}(Y,X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Prooperty 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\text{Cov}(aX,bY)=\\mathbb{E}[aX\\cdot bY]-\\mathbb{E}[aX]\\mathbb{E}[bY]=ab(\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y])=ab\\text{Cov}(X,Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Property 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{aligned}\n",
    "\\text{Cov}(X+a,Y+b)&=\\mathbb{E}[(X+a)(Y+b)]-\\mathbb{E}[X+a]\\mathbb{E}[Y+b] \\\\\n",
    "&=\\mathbb{E}[XY+aY+bX+ab]-\\big(\\mathbb{E}[X]\\mathbb{E}[Y+b]+\\mathbb{E}[a]\\mathbb{E}[Y+b]\\big) \\\\\n",
    "&=\\mathbb{E}[XY]+\\mathbb{E}[aY]+\\mathbb{E}[bX]+\\mathbb{E}[ab]-\\big(\\mathbb{E}[X]\\mathbb{E}[Y]+\\mathbb{E}[X]\\mathbb{E}[b]+\\mathbb{E}[a]\\mathbb{E}[Y]+\\mathbb{E}[a]\\mathbb{E}[b]\\big) \\\\\n",
    "&=\\mathbb{E}[XY]+a\\mathbb{E}[Y]+b\\mathbb{E}[X]+ab-\\big(\\mathbb{E}[X]\\mathbb{E}[Y]+b\\mathbb{E}[X]+a\\mathbb{E}[Y]+ab\\big) \\\\\n",
    "&=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]=\\text{Cov}(X,Y) \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Property 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{aligned}\n",
    "\\text{Cov}(aX+bY,cW+dV)&=\\mathbb{E}[(aX+bY)(cW+dV)]-\\mathbb{E}[aX+bY]\\mathbb{E}[cW+dV] \\\\\n",
    "&=\\mathbb{E}[(aX\\cdot cW+bY\\cdot cW + aX\\cdot dV+bY\\cdot dV)]-\\big(\\mathbb{E}[aX]+\\mathbb{E}[bY]\\big)\\big(\\mathbb{E}[cW]+\\mathbb{E}[dV]\\big) \\\\\n",
    "&=\\mathbb{E}[aX\\cdot cW]+\\mathbb{E}[bY\\cdot cW] + \\mathbb{E}[aX\\cdot dV]+\\mathbb{E}[bY\\cdot dV] \\\\\n",
    "&\\ \\ \\ \\ -\\big(\\mathbb{E}[aX]\\mathbb{E}[cW]+\\mathbb{E}[aX]\\mathbb{E}[dV]+\\mathbb{E}[bY]\\mathbb{E}[cW]+\\mathbb{E}[bY]\\mathbb{E}[dV]\\big) \\\\\n",
    "&=\\big(\\mathbb{E}[aX\\cdot cW]-\\mathbb{E}[aX]\\mathbb{E}[cW]\\big)+\\big(\\mathbb{E}[bY\\cdot cW]-\\mathbb{E}[bY]\\mathbb{E}[cW]\\big)  \\\\\n",
    "&\\ \\ \\ \\ + \\big(\\mathbb{E}[aX\\cdot dV]-\\mathbb{E}[aX]\\mathbb{E}[dV]\\big)+\\big(\\mathbb{E}[bY\\cdot dV]-\\mathbb{E}[bY]\\mathbb{E}[dV]\\big) \\\\\n",
    "&=\\big(\\mathbb{E}[aX\\cdot cW]-\\mathbb{E}[aX]\\mathbb{E}[cW]\\big)+\\big(\\mathbb{E}[aX\\cdot dV]-\\mathbb{E}[aX]\\mathbb{E}[dV]\\big) \\\\\n",
    "&\\ \\ \\ \\ +\\big(\\mathbb{E}[bY\\cdot cW]-\\mathbb{E}[bY]\\mathbb{E}[cW]\\big)+\\big(\\mathbb{E}[bY\\cdot dV]-\\mathbb{E}[bY]\\mathbb{E}[dV]\\big) \\\\\n",
    "&=\\text{Cov}(aX,cW)+\\text{Cov}(aX,dV)+\\text{Cov}(bY,cW)+\\text{Cov}(bY,dV) \\\\\n",
    "&=ac\\text{Cov}(X, W)+ad\\text{Cov}(X,V)+bc\\text{Cov}(Y,W)+bd\\text{Cov}(Y,V) \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T05:29:47.009393Z",
     "start_time": "2020-11-01T05:29:47.007567Z"
    }
   },
   "source": [
    "## Relationship to Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T05:29:47.393586Z",
     "start_time": "2020-11-01T05:29:47.390556Z"
    }
   },
   "source": [
    "$\\text{Cov}(X,X)=\\text{Var}(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T05:29:15.114385Z",
     "start_time": "2020-11-01T05:29:15.110296Z"
    },
    "hidden": true
   },
   "source": [
    "$$\\begin{aligned}\n",
    "\\text{Cov}(X,X)&=\\mathbb{E}[(X-\\mathbb{E}[X])(X-\\mathbb{E}[X])] \\\\ \n",
    "&=\\mathbb{E}[(X-\\mathbb{E}[X])^2] \\\\\n",
    "&=\\text{Var}(X,X)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $X$ and $Y$ are independent, then $\\text{Cov}(X,Y)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If $X$ and $Y$ are independent, then $\\mathbb{E}[XY]=\\mathbb{E}[X]\\mathbb{E}[Y]$ and hence $\\text{Cov}(X,Y)=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[X]=\\mathbb{E}[X]\\mathbb{E}[Y]-\\mathbb{E}[X]\\mathbb{E}[X]=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncorrelatedness does NOT imply independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two random variables $X$ and $Y$ are said to be *uncorrelated* if $\\text{Cov}(X,Y)=0$. The properties of covariance above tell us that if $X$ and $Y$ are independent, then $\\text{Cov}(X,Y)=0$ and hence $X$ and $Y$ are uncorrelated.\n",
    "\n",
    "One immediate question that follows is whether uncorrelatedness implies independence. The simple answer is **no**. We will explore a counter-example to demonstrate that the implication does not hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the random variables $X$ and $Y=X^2$ where $X$ is a [uniform distribution](https://en.wikipedia.org/wiki/Continuous_uniform_distribution) in the range $[-1,1]$. $X$ and $Y$ are both dependent on the random variable $X$ and hence not independent. Now, we need to show that $X$ and $Y$ are uncorrelated.\n",
    "\n",
    "Since I am deferring a discussion of common probability distributions to a future notebook, I will provide tthe required information for the computations. Given constants $a,b\\in\\mathbb{R}$, for a continuous uniform distribution in the range $[a,b]$, $\\forall x\\in\\mathbb{R}$ the PDF is given by \n",
    "$$f(x)=\\begin{cases}0 & x<a \\\\ \\frac{1}{b-a} & a\\le x\\le b \\\\ 0 & x>b\\end{cases}$$\n",
    "\n",
    "In addition, we use the [Law of the Unconscious Statistian](https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician) briefly discussed in the \"Advanced_Probability_Basics\" notebook for the computation of $\\mathbb{E}[X^3]$.\n",
    "\n",
    "$\\mathbb{E}[X]=\\int_{-\\infty}^\\infty xf(x)dx=\\int_{-1}^1\\frac{x}{2}dx=\\frac{x^2}{4}\\bigg|_{x=-1}^1=\\frac{1}{4}-\\frac{1}{4}=0$\n",
    "\n",
    "$\\mathbb{E}[X]=\\int_{-1}^1\\frac{x^3}{2}dx=\\frac{x^4}{8}\\bigg|_{x=-1}^1=\\frac{1}{8}-\\frac{1}{8}=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally have all the tools we need to compute the covraince between $X$ and $Y$. We will use the simplified form of the covariance discussed above in order to make our calculations a tad easier.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\text{Cov}(X,Y)&=\\text{Cov}(X,X^2) \\\\ \n",
    "&=\\mathbb{E}[X\\cdot X^2]-\\mathbb{E}[X]\\mathbb{E}[X^2] \\\\\n",
    "&=\\mathbb{E}[X^3]-\\mathbb{E}[X]\\mathbb{E}[X^2] \\\\\n",
    "&=0-0\\cdot\\mathbb{E}[X^2] \\\\\n",
    "&=0 \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we have shown that two random variables can be uncorrelated, but not independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample correlation is called the \"Pearson product-moment correlation coefficient,\" \"Pearson correlation coefficient,\" or \"correlation coefficient.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measure of correlation for sample data $(x_1,y_1),\\cdots,(x_n,y_n)$ for two covariates $x$ and $y$ with sample means $\\bar{x}$ and $\\bar{y}$ is given by\n",
    "\n",
    "$$r_{xy}=\\frac{\\sum\\limits_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum\\limits_{i=1}^n(x_i-\\bar{x})^2}\\sqrt{\\sum\\limits_{i=1}^n(y_i-\\bar{y})^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given two random variables $X$ and $Y$ with expected values $\\mu_X$ and $\\mu_Y$ and standard deviations $\\sigma_X$ and $\\sigma_Y$, the population correlation is given by\n",
    "\n",
    "$$\\rho_{X,Y}=\\text{Corr}(X,Y)=\\frac{\\text{Cov}(X,Y)}{\\sigma_X\\sigma_Y}=\\frac{\\mathbb{E}\\big[(X-\\mu_X)(Y-\\mu_Y)\\big]}{\\sigma_X\\sigma_Y}$$\n",
    "\n",
    "$$\\rho_{X,Y}=\\text{Corr}(X,Y)=\\frac{\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]}{\\sqrt{\\mathbb{E}[X^2]-\\mathbb{E}[X]^2}\\sqrt{\\mathbb{E}[Y^2]-\\mathbb{E}[Y]^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\text{Corr}(X,Y)=\\text{Corr}(Y,X)$\n",
    "2. $r_{xy}=r_{yx}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Property 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\text{Corr}(X,Y)=\\frac{\\text{Cov}(X,Y)}{\\sigma_X\\sigma_Y}=\\frac{\\text{Cov}(Y,X)}{\\sigma_Y\\sigma_X}=\\text{Corr}(Y,X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Property 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$r_{xy}=\\frac{\\sum\\limits_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum\\limits_{i=1}^n(x_i-\\bar{x})^2}\\sqrt{\\sum\\limits_{i=1}^n(y_i-\\bar{y})^2}}=\\frac{\\sum\\limits_{i=1}^n(y_i-\\bar{y})(x_i-\\bar{x})}{\\sqrt{\\sum\\limits_{i=1}^n(y_i-\\bar{y})^2\\sqrt{\\sum\\limits_{i=1}^n(x_i-\\bar{x})^2}}}=r_{yx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounds for Sample Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given real-valued data set $(x_1,y_1),\\cdots,(x_n,y_n)$, <br>\n",
    "$-1\\le r_{xy}\\le 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The above bound is a direct application of the [Cauchy-Schwarz Inequality](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Consider the data set $(x_1,y_1),\\cdots,(x_n,y_n)$ with sample mean $(\\bar{x},\\bar{y})$ and standard deviation $(\\sigma_x,\\sigma_y)$.\n",
    "\n",
    "Consider the vectors $\\mathbf{x}=[x_1-\\bar{x},\\cdots,x_n-\\bar{x}]$ and $\\mathbf{y}=[y_1-\\bar{y},\\cdots,y_n-\\bar{y}]$ for the covariates $X$ and $Y$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Given $\\mathbf{a},\\mathbf{b}\\in\\mathbb{R}^n$, denote $\\langle\\mathbf{a},\\mathbf{b}\\rangle$ denote the inner product of vectors $\\mathbf{a}$ and $\\mathbf{b}$. We will use the following computations in our discussion below.\n",
    "\n",
    "$$||\\mathbf{x}||=\\sqrt{\\langle\\mathbf{x},\\mathbf{x}\\rangle}=\\sqrt{\\sum\\limits_{i=1}^n(x_i-\\bar{x})^2}$$\n",
    "\n",
    "$$||\\mathbf{y}||=\\sqrt{\\langle\\mathbf{y},\\mathbf{y}\\rangle}=\\sqrt{\\sum\\limits_{i=1}^n(y_i-\\bar{y})^2}$$\n",
    "\n",
    "$$\\langle\\mathbf{x},\\mathbf{y}\\rangle=\\sum\\limits_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The Caucy-Schwarz Inequality states that $\\forall\\mathbf{a},\\mathbf{b}\\in\\mathbb{R}^n$ $|\\langle\\mathbf{a},\\mathbf{b}\\rangle|\\le||\\mathbf{a}||\\cdot||\\mathbf{b}||\\implies -1\\le\\frac{\\langle\\mathbf{a},\\mathbf{b}\\rangle}{||\\mathbf{a}||\\cdot||\\mathbf{b}||}\\le1$\n",
    "\n",
    "Plugging in our computations above, we obtain\n",
    "\n",
    "$-1\\le r_{xy}=\\frac{\\langle\\mathbf{x},\\mathbf{y}\\rangle}{||\\mathbf{x}||\\cdot||\\mathbf{y}||}\\le1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounds for Population Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given real-valued random variables $X$ and $Y$ with joint distribution $(X,Y)$,\n",
    "\n",
    "$-1\\le \\rho_{X,Y}\\le 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Lemma: Given real Given real-valued random variables $X$ and $Y$ with joint distribution $(X,Y)$, with means $(\\mu_X,\\mu_Y)$ and variances $(\\sigma_X^2,\\sigma_Y^2)$, the covariance $\\text{Cov}(X,Y)$ exists.** <br><br>\n",
    "\n",
    "The above statement is a direct result of the [Cauchy-Schwarz Inequality](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality) that states $|\\text{Cov}(X,Y)|\\le\\sqrt{\\text{Var}(X)\\text{Var}(Y)}$. Since we have clear bounds for the value of $\\text{Cov}(X,Y)$, it definitely exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Claim: Given real-valued random variables $X$ and $Y$ with joint distribution $(X,Y)$, with means $(\\mu_X,\\mu_Y)$ and variances $(\\sigma_X^2,\\sigma_Y^2)$ <br>\n",
    "$-1\\le \\rho_{X,Y}\\le 1$** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As a side note, I really enjoy the proof below because it highlights the creativity often required to show mathematical properties in an elegant fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Consider two real-valued random variables $X$ and $Y$ with joint distribution $(X,Y)$. Furthermore, $X$ has mean $\\mu_X$ and variance $\\sigma^2_X$, while $Y$ has mean $\\mu_Y$ and variance $\\sigma^2_Y$.\n",
    "\n",
    "Now, consider the function below for some $t\\in\\mathbb{R}$. \n",
    "\n",
    "$$\\begin{aligned}\n",
    "g(t)&=\\text{Var}(Xt+Y)=\\mathbb{E}\\bigg[\\big((X-\\mu_X)t+(Y-\\mu_Y)\\big)^2\\bigg] \\\\\n",
    "&=\\mathbb{E}\\big[(X-\\mu_X)^2t^2+2t(X-\\mu_X)(Y-\\mu_Y)+(Y-\\mu_Y)^2\\big] \\\\\n",
    "&=\\sigma_X^2t^2+2t\\text{Cov}(X,Y)+\\sigma_Y^2 \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order for us to employ the non-negativity property of the variance, we first need to show that $\\text{Var}(Xt+Y)$ exists. Since the variances for $X$ and $Y$ exist, we know $\\text{Cov}(X,Y)$ exists (by our lemma above) and have an exact relation for the variance shown below.\n",
    "$$\\text{Var}(Xt+Y)=t^2\\text{Var}(X)+\\text{Var}(Y)+2t\\text{Cov}(X,Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T20:11:15.697370Z",
     "start_time": "2020-11-01T20:11:15.692593Z"
    },
    "hidden": true
   },
   "source": [
    "Since variance is non-negative and the above is a quadratic function with respect to $t$, we know the discrinimant must be greater than or equal to 0. In other words,\n",
    "\n",
    "$(2\\text{Cov}(X,Y))^2-4\\sigma_X^2\\sigma_Y^2=4\\text{Cov}^2(X,Y)-4\\sigma_X^2\\sigma_Y^2\\ge 0$\n",
    "\n",
    "$\\implies 1\\le\\bigg(\\frac{\\text{Cov}(X,Y)}{\\sigma_X\\sigma_Y}\\bigg)^2=\\rho_{X,Y}^2$\n",
    "\n",
    "$\\therefore -1\\le\\rho_{X,Y}\\le1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T16:26:31.677667Z",
     "start_time": "2020-11-01T16:26:31.673282Z"
    }
   },
   "source": [
    "The Pearson correlation coefficient plays an important part in the interpretation of linear regression, so I will defere a more detailed discussion of $r_{xy}^2$ to a future notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the correlation between two covariates or random variables is bounded between $[-1,1]$, this can offer a measure of relatedness between two variables that may be easier to interpret than its cousin, the corvariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to covariance, if two random variables $X$ and $Y$, part of a joint distribution $(X,Y)$ with variances $\\text{Var}(X)$ and $\\text{Var}(Y)$, are independent, then $\\text{Corr}(X,Y)=0$. However, if two variables are uncorrelated, we **cannot** say whether $X$ and $Y$ are independent. The mathematical discussion to show this fact is nearly identical to the case of covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T18:55:40.537298Z",
     "start_time": "2020-10-29T18:55:40.535352Z"
    }
   },
   "source": [
    "# Indicator Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a subset $A$ of set $X$ (i.e. $A\\subseteq X$), the indicator function $\\mathbb{1}_A$ is a mapping defined below.\n",
    "\n",
    "$$\\mathbb{1}:X\\to\\{0,1\\}$$\n",
    "\n",
    "$$\\mathbb{1}_A(x)=\\begin{cases}1  & if\\ x\\in A \\\\ 0 & if\\ x\\notin A \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T18:55:52.883579Z",
     "start_time": "2020-10-29T18:55:52.881476Z"
    }
   },
   "source": [
    "# Properties of Indicator Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T19:10:16.660320Z",
     "start_time": "2020-10-29T19:10:16.658110Z"
    }
   },
   "source": [
    "## Indicator Function for Intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\forall A,B\\in X$, $\\forall\\alpha\\in X$, $\\mathbb{1}_{A\\cap B}(\\alpha)=\\min\\{\\mathbb{1}_A(\\alpha),\\mathbb{1}_B(\\alpha)\\}=\\mathbb{1}_A(\\alpha)\\cdot\\mathbb{1}_B(\\alpha)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\forall\\alpha\\in X$, $\\mathbb{1}_{A\\cap B}(\\alpha)=\\begin{cases}1  & if\\ \\alpha\\in A\\cap B \\\\ 0 & if\\ \\alpha\\notin A\\cap B \\end{cases}$\n",
    "\n",
    "There are four cases to examine depending on whether a given element $\\alpha\\in X$ is in $A$ or $B$.\n",
    "\n",
    "1. If  $\\alpha\\in A$ and $\\alpha\\in B$, then $\\alpha\\in A\\cap B$.\n",
    "\n",
    "$$\\mathbb{1}_{A\\cap B}(\\alpha)=1$$\n",
    "$$\\min\\{\\mathbb{1}_A(\\alpha),\\mathbb{1}_B(\\alpha)\\}=\\min\\{1,1\\}=1$$\n",
    "$$\\mathbb{1}_A(\\alpha)\\cdot\\mathbb{1}_B(\\alpha)=1\\cdot 1=1$$\n",
    "2. If $\\alpha\\in A$ and $\\alpha\\notin B$, then $\\alpha\\notin A\\cap B$\n",
    "$$\\mathbb{1}_{A\\cap B}(\\alpha)=0$$\n",
    "$$\\min\\{\\mathbb{1}_A(\\alpha),\\mathbb{1}_B(\\alpha)\\}=\\min\\{1,0\\}=0$$\n",
    "$$\\mathbb{1}_A(\\alpha)\\cdot\\mathbb{1}_B(\\alpha)=1\\cdot 0=0$$\n",
    "3. If $\\alpha\\notin A$ and $\\alpha\\in B$, then $\\alpha\\notin A\\cap B$\n",
    "$$\\mathbb{1}_{A\\cap B}(\\alpha)=0$$\n",
    "$$\\min\\{\\mathbb{1}_A(\\alpha),\\mathbb{1}_B(\\alpha)\\}=\\min\\{0,1\\}=0$$\n",
    "$$\\mathbb{1}_A(\\alpha)\\cdot\\mathbb{1}_B(\\alpha)=0\\cdot 1=0$$\n",
    "4. If $\\alpha\\notin A$ and $\\alpha\\notin B$, then $\\alpha\\notin A\\cap B$\n",
    "$$\\mathbb{1}_{A\\cap B}(\\alpha)=0$$\n",
    "$$\\min\\{\\mathbb{1}_A(\\alpha),\\mathbb{1}_B(\\alpha)\\}=\\min\\{0,0\\}=0$$\n",
    "$$\\mathbb{1}_A(\\alpha)\\cdot\\mathbb{1}_B(\\alpha)=0\\cdot 0=0$$\n",
    "\n",
    "$\\therefore\\forall A,B\\subseteq X$, $\\forall\\alpha\\in X$ $\\mathbb{1}_{A\\cap B}(\\alpha)=\\min\\{\\mathbb{1}_A(\\alpha),\\mathbb{1}_B(\\alpha)\\}=\\mathbb{1}_A(\\alpha)\\cdot\\mathbb{1}_B(\\alpha)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator Function for Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\forall A,B\\in X$, $\\forall\\alpha\\in X$, $\\mathbb{1}_{A\\cup B}(\\alpha)=\\max\\{\\mathbb{1}_A(\\alpha),\\mathbb{1}_B(\\alpha)\\}=\\mathbb{1}_A(\\alpha)+\\mathbb{1}_B(\\alpha)-\\mathbb{1}_A(\\alpha)\\cdot\\mathbb{1}_B(\\alpha)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\forall\\alpha\\in X$, $\\mathbb{1}_{A\\cup B}(\\alpha)=\\begin{cases}1  & if\\ \\alpha\\in A\\cup B \\\\ 0 & if\\ \\alpha\\notin A\\cup B \\end{cases}$ <br>\n",
    "There are four cases to examine depending on whether a given element $\\alpha\\in X$ is in $A$ or $B$. <br>\n",
    "1. If  $\\alpha\\in A$ and $\\alpha\\in B$, then $\\alpha\\in A\\cup B$. <br>\n",
    "$$\\mathbb{1}_{A\\cup B}(\\alpha)=1$$\n",
    "$$\\max\\{\\mathbb{1}_A(\\alpha),\\mathbb{1}_B(\\alpha)\\}=\\max\\{1,1\\}=1$$\n",
    "$$\\mathbb{1}_A(\\alpha)+\\mathbb{1}_B(\\alpha)-\\mathbb{1}_A(\\alpha)\\cdot\\mathbb{1}_B(\\alpha)=1+1-1\\cdot 1=1$$\n",
    "2. If $\\alpha\\in A$ and $\\alpha\\notin B$, then $\\alpha\\in A\\cup B$\n",
    "$$\\mathbb{1}_{A\\cup B}(\\alpha)=1$$\n",
    "$$\\max\\{\\mathbb{1}_A(\\alpha),\\mathbb{1}_B(\\alpha)\\}=\\max\\{1,0\\}=1$$\n",
    "$$\\mathbb{1}_A(\\alpha)+\\mathbb{1}_B(\\alpha)-\\mathbb{1}_A(\\alpha)\\cdot\\mathbb{1}_B(\\alpha)=1+0-1\\cdot 0=1$$\n",
    "3. If $\\alpha\\notin A$ and $\\alpha\\in B$, then $\\alpha\\in A\\cup B$\n",
    "$$\\mathbb{1}_{A\\cup B}(\\alpha)=1$$\n",
    "$$\\max\\{\\mathbb{1}_A(\\alpha),\\mathbb{1}_B(\\alpha)\\}=\\max\\{0,1\\}=1$$\n",
    "$$\\mathbb{1}_A(\\alpha)+\\mathbb{1}_B(\\alpha)-\\mathbb{1}_A(\\alpha)\\cdot\\mathbb{1}_B(\\alpha)=0+1-0\\cdot 1=1$$\n",
    "4. If $\\alpha\\notin A$ and $\\alpha\\notin B$, then $\\alpha\\notin A\\cup B$\n",
    "$$\\mathbb{1}_{A\\cup B}(\\alpha)=0$$\n",
    "$$\\max\\{\\mathbb{1}_A(\\alpha),\\mathbb{1}_B(\\alpha)\\}=\\max\\{0,0\\}=0$$\n",
    "$$\\mathbb{1}_A(\\alpha)+\\mathbb{1}_B(\\alpha)-\\mathbb{1}_A(\\alpha)\\cdot\\mathbb{1}_B(\\alpha)=0+0-0\\cdot 0=0$$\n",
    "\n",
    "$\\therefore\\forall A,B\\in X$, $\\forall\\alpha\\in X$ $\\mathbb{1}_{A\\cup B}(\\alpha)=\\max\\{\\mathbb{1}_A(\\alpha),\\mathbb{1}_B(\\alpha)\\}=\\mathbb{1}_A(\\alpha)+\\mathbb{1}_B(\\alpha)-\\mathbb{1}_A(\\alpha)\\cdot\\mathbb{1}_B(\\alpha)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator Function of Complement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an event $A\\subseteq\\Omega$ and its complement $A^c=\\Omega\\backslash A$, $\\forall\\omega\\in A^c$, $\\mathbb{1}_{A^c}=1-\\mathbb{1}_A(\\omega)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\forall\\omega\\in\\Omega$, if $\\omega\\in A$, then by definition $\\omega\\notin A$.\n",
    "\n",
    "$$\\mathbb{1}_{A^c}(\\omega)=0$$\n",
    "\n",
    "$$1-\\mathbb{1}_A(\\omega)=1-1=0$$\n",
    "\n",
    "Now suppose $\\omega\\notin A$, which means that $\\omega\\in A$.\n",
    "\n",
    "$$\\mathbb{1}_{A^c}(\\omega)=1$$\n",
    "\n",
    "$$1-\\mathbb{1}_A(\\omega)=1-0=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Powers of Indicator Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\forall n\\in\\mathbb{R}\\backslash 0$, given an event $A\\subseteq\\Omega$, $\\forall\\omega\\in A$, $\\big(\\mathbb{1}_A(\\omega)\\big)^n=\\mathbb{1}_A(\\omega)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For some $\\omega\\in\\Omega$, suppose $\\omega\\in A$.\n",
    "\n",
    "$$\\big(\\mathbb{1}_A(\\omega)\\big)^n=1^n=1=\\mathbb{1}_A(\\omega)$$\n",
    "Now suppose $\\omega\\notin A$.\n",
    "\n",
    "$$\\big(\\mathbb{1}_A(\\omega)\\big)^n=0^n=0=\\mathbb{1}_A(\\omega)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T20:12:18.036025Z",
     "start_time": "2020-10-29T20:12:18.033960Z"
    }
   },
   "source": [
    "# Mean, Variance and Covariance of Indicator Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following discussion, assume we are given a [probability space](https://en.wikipedia.org/wiki/Probability_space) $(\\Omega,\\cal{F},P)$ and an event $A\\in\\cal{F}$. The indicator function $\\mathbb{1}_A$ is defined below for the probability space. <br>\n",
    "$$\\mathbb{1}_A:\\Omega\\to\\mathbb{R}$$\n",
    "$$\\mathbb{1}_A(\\omega)=\\begin{cases}1 & \\omega\\in A \\\\ 0 & \\omega\\notin A\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T20:16:49.438195Z",
     "start_time": "2020-10-29T20:16:49.436221Z"
    }
   },
   "source": [
    "## Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{E}[\\mathbb{1}_A]=P(A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In case you decide to peruse the proof below, I want to make an important distinction between functions $p$ and $P$ in the context of probability spaces. While $P$ is a function that maps a given event $E\\in\\cal{F}$ to $[0,1]$, $p$ is a function that maps a given outcome $\\omega\\in\\Omega$ to $[0,1]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Lemma: Given an event $A\\subseteq \\Omega$, $P(A)=\\sum\\limits_{\\omega\\in A} p(\\omega)$**\n",
    "\n",
    "\n",
    "Given an element $\\omega\\in\\Omega$ and an event $E_\\omega=\\{\\omega\\}$, $P(E_\\omega)=p(\\omega)$. $\\forall\\omega\\in A$, we define the disjoint events $E_\\omega$ (the events do not share any outcomes and are thus disjoint) and then employ the $\\sigma$-additivity axiom of probability.\n",
    "\n",
    "$$P(A)=P\\bigg(\\bigcup\\limits_{\\omega\\in A}E_\\omega\\bigg)=\\sum\\limits_{\\omega\\in A}P(E_\\omega)=\\sum\\limits_{\\omega\\in A}p(\\omega)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Claim: $\\forall\\omega\\in\\Omega$,  $\\mathbb{E}[\\mathbb{1}_A]=P(A)$**\n",
    "\n",
    "\n",
    "We will use the fact that $\\forall\\omega\\in\\Omega$, $\\omega\\in A$ or $\\omega\\notin A$ in the proof below.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{E}[\\mathbb{1}_A] &= \\sum\\limits_{\\omega\\in\\Omega}p(\\omega)\\mathbb{1}_A(\\omega) \\\\\n",
    "&=\\sum\\limits_{\\omega_i\\in A}p(\\omega_i)\\mathbb{1}_A(\\omega_i)+\\sum\\limits_{\\omega_j\\notin A}p(\\omega_j)\\mathbb{1}_A(\\omega_j) \\\\\n",
    "&=\\sum\\limits_{\\omega_i\\in A}p(\\omega_i)\\cdot 1+\\sum\\limits_{\\omega_i\\notin A}p(\\omega_i)\\cdot 0 \\\\\n",
    "&=\\sum\\limits_{\\omega_i\\in A}p(\\omega_i) \\\\\n",
    "&=P(A)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T20:16:58.366304Z",
     "start_time": "2020-10-29T20:16:58.364440Z"
    }
   },
   "source": [
    "## Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Var}(\\mathbb{1}_A)=P(A)(1-P(A))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{aligned}\n",
    "\\text{Var}(\\mathbb{1}_A)&=\\mathbb{E}\\bigg[\\big(\\mathbb{1}_A-\\mathbb{E}[\\mathbb{1}_A]\\big)^2\\bigg]\\\\\n",
    "&=\\mathbb{E}\\bigg[\\big(\\mathbb{1}_A-P(A)\\big)^2\\bigg] \\\\\n",
    "&=\\mathbb{E}\\big[\\mathbb{1}_A^2-2P(A)\\mathbb{1}_A+P(A)^2\\big] \\\\\n",
    "&=\\mathbb{E}\\big[\\mathbb{1}_A^2\\big]-\\mathbb{E}\\big[2P(A)\\mathbb{1}_A\\big]+\\mathbb{E}\\big[P(A)^2\\big] \\\\\n",
    "&=\\mathbb{E}[\\mathbb{1}_A]-2P(A)\\mathbb{E}[\\mathbb{1}_A]+P(A)^2 \\\\\n",
    "&=P(A)-2P(A)^2+P(A)^2=P(A)-P(A)^2=P(A)(1-P(A))\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given two events $A,B\\subseteq\\Omega$, $\\text{Cov}\\big(1_A,\\mathbb{1}_B\\big)=P(A\\cap B)-P(A)P(B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{aligned}\n",
    "\\text{Cov}\\big(\\mathbb{1}_A,\\mathbb{1}_B\\big)&=\\mathbb{E}\\bigg[\\big(\\mathbb{1}_A-\\mathbb{E}[\\mathbb{1}_A]\\big)\\big(\\mathbb{1}_B-\\mathbb{E}[\\mathbb{1}_B]\\big)\\bigg] \\\\\n",
    "&=\\mathbb{E}\\bigg[\\big(\\mathbb{1}_A-P(A)\\big)\\big(\\mathbb{1}_B-P(B)\\big)\\bigg] \\\\\n",
    "&=\\mathbb{E}\\bigg[\\mathbb{1}_A\\cdot\\mathbb{1}_B -\\mathbb{1}_A P(B)-P(A)\\mathbb{1}_B + P(A)P(B)\\bigg] \\\\\n",
    "&=\\mathbb{E}\\big[\\mathbb{1}_A\\cdot\\mathbb{1}_B\\big] -\\mathbb{E}\\big[\\mathbb{1}_A P(B)\\big]-\\mathbb{E}\\big[P(A)\\mathbb{1}_B\\big] + \\mathbb{E}\\big[P(A)P(B)\\big] \\\\\n",
    "&=\\mathbb{E}\\big[\\mathbb{1}_{A\\cap B}\\big] -P(B)\\mathbb{E}\\big[\\mathbb{1}_A \\big]-P(A)\\mathbb{E}\\big[\\mathbb{1}_B\\big] + P(A)P(B) \\\\\n",
    "&=P(A\\cap B) -P(A)P(B)-P(A)P(B) + P(A)P(B) \\\\\n",
    "&=P(A\\cap B) -P(A)P(B) \\\\\n",
    "\\end{aligned}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
